{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b119fa72-6a7d-43f8-b9f7-d31a34e4e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c0371b-c773-4bc2-a040-9c6b19f33c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b1a10-4f59-4cd3-8f48-cf3eec1425f1",
   "metadata": {},
   "source": [
    "# Banana Collection DRL Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8a4ca-6c12-497c-913f-2b97a6b287ba",
   "metadata": {},
   "source": [
    "This report presents the experiment results collected during the [Udacity DRL Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)\n",
    "[Project 1 - Navigation.](https://github.com/udacity/deep-reinforcement-learning/tree/master/p1_navigation). This report is organized as follows:\n",
    "1) [Algorithm Details](#Algorithm-Details)\n",
    "1) [Experimental Approach](#Experimental-Approach)\n",
    "1) [Model Results](#Model-Results)\n",
    "1) [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa95d0-401c-4b48-a276-2d40122350fa",
   "metadata": {},
   "source": [
    "## Algorithm Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37060c-f7c6-41b0-adaa-971d736e7ceb",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217cae3-c9c7-4caf-979d-6b42ee389078",
   "metadata": {},
   "source": [
    "The environment, described in detail [here](https://github.com/joeworsh/drl_agent_navigation/blob/main/README.md), is well suited for a Deep Reinforcement Learning (DRL) approach. Reinforcement Learning, is a form of machine learning that is designed to optimize policies (i.e. decision makers) which know how to take optimal actions within an environment to reap some optimal reward or return from the environment. Mathematically we can represent the environment and the policy as:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{1}\n",
    "(s_{t+1}, r_{t+1}) = env(s_t, a_t),\n",
    "$$\n",
    "$$\\tag{2}\n",
    "a_t = \\pi(s_t).\n",
    "$$\n",
    "</div>\n",
    "where:\n",
    "\n",
    "* $s_t$ is the environment state at time $t$\n",
    "* $a_t$ is the action performed at time $t$\n",
    "* $r_t$ is the reward from the environment received at time $t$\n",
    "* $\\pi$ denotes the policy which decides on the action to perform given the current environment state\n",
    "\n",
    "This environment will run from time $0 \\leq t \\leq T$ with $T$ denoting the final timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec3657-c777-4373-82ab-710135094d5c",
   "metadata": {},
   "source": [
    "The objective is to find an optimal policy, denoted $\\pi_*$, which maximizes the cumulative reward received over all time steps. This is more than just picking an action that will return the best $r_t$ at every timestep $t$, the agent must consider future rewards as well. There are instances where an undesirable action may lead to a larger reward many states in the future. It is important to optimize and that cumulative reward to learn more realistic and better performing policies. The cumulative return is defined at each timestep $t$ as the current reward and all future rewards (not including past rewards):\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{3}\n",
    "G_t=\\sum_{i=0}^{T-t} r_{t+i}.\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d551d06-1afb-490c-b1c3-787384760212",
   "metadata": {},
   "source": [
    "The difficulty here is that in practice we don't know these future rewards. The only thing we know with certainty is the past and current rewards. The best we can do here is predict the expected rewards the agent will receive if it were to follow a known policy $\\pi$. This concept is denoted mathematically as:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{4}\n",
    "\\tilde{G}_t=\\mathbb{E}_\\pi\\left[\\sum_{i=0}^{T-t} r_{\\pi, t+i}\\right]\n",
    "$$\n",
    "</div>\n",
    "where $r_{\\pi, i}$ is the reward received from $env(s_i, \\pi(s_i))$. Here the policy $\\pi$ is being used to anticipate the future states, and an expectation is used to represent uncertainty about those future returns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c402a8-f96f-444a-89da-e2ef7eb6fa64",
   "metadata": {},
   "source": [
    "One important addition must be made in order to handle this uncertainty: how confident are we in our ability to predict the future rewards? If we are very confident then it would make sense to consider all future rewards with equal weighting to ensure we getting the highest cumulative reward possible. However, if we're not confident in our ability to predict future rewards we need to consider that error to ensure we don't act too confidently when we're wrong. This is addressed with a discounting factor $0 \\leq \\gamma \\leq 1$ which is exponentially applied at each timestep as:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{5}\n",
    "\\tilde{G}_t=\\mathbb{E}_\\pi\\left[\\sum_{i=0}^{T-t} \\gamma^i r_{\\pi, t+i}\\right].\n",
    "$$\n",
    "</div>\n",
    "The farther this equations gets from the current time $t$ the smaller this discount factor gets. The discount is visualized below for several different values of $\\gamma$ up $1000$ timesteps forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46186358-bf50-4ee2-aaec-151b310f3277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "g_vals = [1.0, 0.999, 0.99, 0.9, 0.5, 0.1, 0.0]\n",
    "g = np.array(g_vals).reshape([-1, 1])\n",
    "g = np.tile(g, [1, 1000])\n",
    "g = np.cumprod(g, axis=1)\n",
    "for g_val, g_axis in zip(g_vals, g):\n",
    "    plt.plot(g_axis, label=f\"$\\gamma={g_val}$\")\n",
    "plt.xlabel(\"Time into future $i$\")\n",
    "plt.ylabel(\"$\\gamma$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfb56d-b9fc-4a44-9dce-086e2b3c5310",
   "metadata": {},
   "source": [
    "It is evident here how far into the future each of these $\\gamma$ values will look. The selection of $\\gamma$ is of paramount importance with every Reinforcement Learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e0bc8-15e8-499d-877c-b9ba526f9a24",
   "metadata": {},
   "source": [
    "An important realization emerges while inspecting the formula of $\\tilde{G}_t$ above. This represents the \"score-to-go\" where at the final timestep $T$, $G_T=r_T$ and $G_{T-1}=r_{T-1} + G_T$. This recursive pattern holds true for any value of $t$: $G_t=r_t+G_{t+1}$. This recursive pattern is the same pattern that is used in [Dynammic Programming](https://en.wikipedia.org/wiki/Dynamic_programming) (DP), which is the discipline of optimizing sequential (dynamic) problems. In DP this is called the [Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation) and is traditionally represented as the <b>value $v$</b> of a state. In the RL paradigm the value of a state (with discounting) is defined as:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{6}\n",
    "v_\\pi(s)=\\mathbb{E}_\\pi\\left[r_t + \\gamma v_\\pi(s_{t+1}) \\; \\middle| \\; s_t=s\\right].\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ea543-cf6d-4ff2-8fe7-a469c35e2b8b",
   "metadata": {},
   "source": [
    "<a name=\"gamma\"></a>This value function is important, but it still doesn't give any information on which action should be selected at any state. The value function can be adapted to consider the value of an action at any given state easily. This is called the <b>action-value</b> function, <b>$q$</b>:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{7}\n",
    "q_\\pi(s, a)=\\mathbb{E}_\\pi\\left[r_t + \\gamma \\; \\underset{a_{t+1}}{\\text{max}}\\:q_\\pi(s_{t+1}, a_{t+1}) \\; \\middle| \\; s_t=s, a_t=a\\right].\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903cf20-0b5b-4b11-9b0c-2ef3af5b5123",
   "metadata": {},
   "source": [
    "$q_\\pi$ tells us the expected cumulative return of selecting action $a$ given state $s$ and then continuing to follow policy $\\pi$ until the episode ends. It accomplishes this was the recurrent relation from the Bellman Equation. The importance of this equation is that it identifies what truely needs to be learned for any decision making process. An optimal policy, $\\pi_*$, will always select the action that maximizes it's expected cumulative return:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{8}\n",
    "\\pi_*(s) = \\underset{a}{arg\\:\\text{max}}\\:q(s, a).\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557af8-8651-48c0-bb28-f53308f4a2e5",
   "metadata": {},
   "source": [
    "The objective then is to learn a good representation of the action-value function $q$. If we can learn a good representation of $q$ then we know how to use it to act optimally within the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50d5ae-a875-4a25-a027-f550891c1c51",
   "metadata": {},
   "source": [
    "### Learning $q(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b5f49-9720-4f84-95dd-c4c441b651be",
   "metadata": {},
   "source": [
    "The action-value function can be represented by any parametric or non-parametric mathematical structure that is capable of handling the forms of $s$ and $a$ as inputs and producing a real-valued number as output. This is simple when $s$ and $a$ are both discrete options from a finite set. For example:\n",
    "\n",
    "* $s \\in \\left[1, 2, 3, 4, 5, 6, 7, 8, 9 \\right]$\n",
    "* $a \\in \\left[\\text{move up}, \\text{move down}, \\text{move left}, \\text{move right} \\right]$\n",
    "\n",
    "which can represent discretes cell on a 3x3 grid and options to move in each direction. When states and actions are discrete like this a table can be used where the rows are the different states, the columns are the different actions and the value in each cell is $q(s, a)$. However, a table will not work if one or both of $s$ and $a$ are not integers that come from a finite set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5382f9c-1c93-47d0-9cf7-e14ed45a428c",
   "metadata": {},
   "source": [
    "#### Deep Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933649b8-2416-47a8-a4dd-65c93b9863a2",
   "metadata": {},
   "source": [
    "When the inputs are one or more real-valued numbers, represented by the $n$-dimensional vector $s \\in \\mathbb{R}^n$, this vector would need to be discretized in order to use a table to represent $q(s, a)$. However, it may not be feasible to discretize $s$ in a way that is both sufficiently representative and computationally feasible. In this case a model capable of handling real-valued vector input, such as a [neural network](https://en.wikipedia.org/wiki/Neural_network), is required. In this case the input to the neural network is the real-valued $n$-dimensional vector $s$ and it has an output node for each of the $m$ actions. The value in each of the output cells corresponds to the action-value for the input cell and the associated action. An example of this neural network architecture is shown below:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/nn_q_example.png\" alt=\"Neural Q-Network architecture example\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b5d22-f962-4658-a3ce-d3fad3fcf144",
   "metadata": {},
   "source": [
    "Neural networks are parameterized by a set of learnable weights and biases denoted as the learnable parameters $\\theta$. These weights allow the network to represent any function that is intended to be learned in a machine learning problem. The correct values of $\\theta$ are learned incrementally by computing a loss, that evaluates how well a neural network is performing, and then making small updates to $\\theta$ that will improve the loss. Overtime these values will converge on an optimized set of parameters that enable the neural network to make intelligent decisions related to the problem it was trained on. When using a parameterized model, like a neural network, the parameters $\\theta$ will be shown as an input to the action-value function to indicate which model is being used to compute the value: $q(s, a; \\theta)$.\n",
    "\n",
    "In RL, specifically Q-Learning, the loss is defined as:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{9}\n",
    "\\ell(\\theta)=\\mathbb{E}_{(s,a,r,s') \\sim p(D)}\\left[\\left(r + \\gamma \\; \\underset{a'}{\\text{max}}\\:q(s', a'; \\theta^{-}) - q(s, a; \\theta)\\right)^2\\right].\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979caaaa-1b4a-49cc-b005-cb8b29408e30",
   "metadata": {},
   "source": [
    "This loss can seem intimidating at first but it can be simply broken down and will begin to look like a standard [mean-squared-error](https://en.wikipedia.org/wiki/Mean_squared_error) which is common in any machine learning or regression problem.\n",
    "\n",
    "<a name=\"prioritized_replay\"></a>First, observe that the inputs to the action-value function in the loss function, $(s, a, r, s')$ are sample from a dataset $D$ distributed according to some probability of the samples within dataset $D$. We assume that we have a dataset of experiences, capturing previously seens states, the action that was performed at the state, the reward that was received and the state that followed. This tuple represents a single experience that we wish to learn from. As we engage with the environment we are building this dataset with real experiences that our model is having. The likelihood of this dataset, $p(D)$ helps to determine which experiences we should learn from. This could be a simple uniform distribution, i.e. $p(D) \\triangleq U(d)$, or it adapt to give a higher likelihood to experiences that the model is not as familiar with. The latter is called <i>Prioritized Experience Replay</i> [[1]](#1) and it uses the magnitude of the loss above to decide how familiar the model is with that experience. When the loss is larger for some sample it will be more likely to see that sample again. The $\\mathbb{E}_{(s,a,r,s') \\sim p(D)}$ component of the loss function is simply showing that while learning the experiences we are learning on are being sampled from that dataset.\n",
    "\n",
    "The next component to understand is $q(s, a; \\theta)$ which tells us what the model, parameterized by $\\theta$, thinks the action-value function for $s$ and $a$ should be. This is ultimately the value we hope our model will be able to produce accurately. It is used here as the prediction input to a mean-squared-error loss function.\n",
    "\n",
    "Finally, $r + \\gamma \\; \\underset{a'}{\\text{max}}\\:q(s', a'; \\theta^{-})$ acts as an approximation of the actual value we hope our model will learn to represent. Ultimately we hope that our model will learn the true cumulative reward that follows any state action pair - however it would be computationally inefficient if we were to simulate full episodes and train only on sequential experiences. It is far more important that we can learn from individual experiences which is why the cumulative return is approximated by the known reward given for an experience and the approximated return of the next state: $\\underset{a'}{\\text{max}}\\:q(s', a'; \\theta^{-})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351be38-335f-48c8-ad6f-ac8b90ab7986",
   "metadata": {},
   "source": [
    "##### Target Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda22f7d-2486-4247-9512-dcad0b8c7a02",
   "metadata": {},
   "source": [
    "Notice that in the loss function above there are two sets of learnable parameters present: $\\theta$ and $\\theta^-$. This is an important feature of the Deep Q-Network (DQN) [[2]](#2) which helps to address an important problem that causes divergence while training. If $\\theta$ were used directly to compute both the label and prediction, as described above, it would be equivalent to trying to optimize on a moving target. It's as if the model is a dog chasing the its own tail: as it gets closer to its tail the tail pulls away. Interestingly Mnih et al [[2]](#2) showed that this circular dependency can be broken by simply creating a <i>target</i> network that lags slightly behind the learned model, denoted $\\theta^-$. An additional hyperparameter is added that controls how often the target network parameters $\\theta^-$ are updated towards the actual $\\theta$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392eaac-4a1c-4b1d-83d2-df93699f85f7",
   "metadata": {},
   "source": [
    "##### Double Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c7d5d-b8e5-41fd-aad5-f2941d2b496c",
   "metadata": {},
   "source": [
    "An important discovery regarding DQN performance, presented by Van Hasselt et al [[3]](#3), is that the DQN is susceptable to overestimating the value of actions. This overestimation can lead to poor performance during training because it always selects the action with the highest estimated value when computing the loss. Overestimation can destabilize training and cause the learner to pursue policies that are not optimal in the context of the environment. One way to address this overestimation is regularize the estimating with another Q model that must learn to agree on which actions are best. It easy to see how Equation 7 can be rewritten to split the <i>max</i> operation into a call to $q$ and an <i>argmax</i> call:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{10}\n",
    "q_\\pi(s, a)=\\mathbb{E}_\\pi\\left[r_t + \\gamma \\; q(s_{t+1}, \\underset{a}{arg\\:\\text{max}}\\:q(s_{t+1}, a; \\theta); \\theta^{-}) \\; \\middle| \\; s_t=s, a_t=a\\right].\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7dd4bb-6cc8-4df8-89d1-580ff3a59c69",
   "metadata": {},
   "source": [
    "Note that the primary network, called the online network, is used to decide which action is best but the target network, $\\theta^{-}$, is used to estimate the value of the selected action. It has been proven that this discontinuation is powerful enough to help stabilize the learning process [[3]](#3). When put into the loss function it now looks like:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{11}\n",
    "\\ell(\\theta)=\\mathbb{E}_{(s,a,r,s') \\sim p(D)}\\left[\\left(r + \\gamma \\; q(s', \\underset{a'}{arg\\:\\text{max}}\\:q(s', a'; \\theta); \\theta^{-}) - q(s, a; \\theta)\\right)^2\\right].\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c1926-b2c1-4b21-8d0e-d69b6327dfea",
   "metadata": {},
   "source": [
    "##### Dueling Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef1b0e-6823-4cf6-a5e4-41aae48f134c",
   "metadata": {},
   "source": [
    "One final improvement to the vanilla DQN will be mentioned here: the dueling DQN [[4]](#4). In the RL literature the value of a state $v(s)$ is related to the action-value $q(s, a)$ through the <i>advantage</i>, $A(s, a)$ of the action $a$ in state $s$:\n",
    "<div style=\"font-size:16px\">\n",
    "$$\\tag{12}\n",
    "q(s, a)=v(s)+A(s, a).\n",
    "$$\n",
    "</div>\n",
    "In the same way that creating a Double DQN helps to eliminate internal interdependency to stabilize training, a network can be updated to independently predict both the value and advantage and then combine them to produce $q(s, a)$ [[4]](#4). An example of a Dueling DQN architecture is shown below:\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/deuling_dqn_architecture.png\" alt=\"Deuling DQN Architecture\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a725355-c4c8-4357-966a-596ef268c3d3",
   "metadata": {},
   "source": [
    "## Experimental Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dfe973-1abc-49c3-91dc-d2d64f10093b",
   "metadata": {},
   "source": [
    "In this project, a DQN [[2]](#2) with options to support Prioritized Replay [[1]](#1), Double DQN [[3]](#3) and Dueling DQN [[4]](#4) is implemented using only [PyTorch](https://pytorch.org/). The objective of this project is to train an agent that is capable of the winning the Banana Collection game by getting an average score of 13 or greater over 100 episodes. The way to achieve this goal is to implement the code correctly and find a set of hyperparameter values that enables good learning for this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3c09d-8820-49b4-8c33-5c8ef8f2df5c",
   "metadata": {},
   "source": [
    "The hyperparameters that may be tuned for this model are as follows:\n",
    "* <code>hidden_layers</code>: comma-separated list of integers describing the number of nodes in each layer (where the length of the list describes the number of layers). Defaults to <code>[256,]</code>.\n",
    "* <code>episodes</code>: The number of episodes to train over. Defaults to <code>10000</code>.\n",
    "* <code>batch_size</code>: The number of (s, a, r, s') experiences included in each mini-batch. Defaults to <code>128</code>.\n",
    "* <code>buffer_size</code>: The experience replay buffer is first-in-first-out structure. This parameter sets the maximum number of entires that can be in the buffer at the same time. Defaults to <code>10000</code>.\n",
    "* <code>update_rate</code>: Sets the rate, in steps, at which the online network parameters $\\theta$ are trained. Defaults to <code>10</code>.\n",
    "* <code>learning_rate</code>: Sets the step size to use during stochastic gradient descent updates. Defaults to <code>1e-3</code>.\n",
    "* <code>discount_rate</code>: Sets the value of $\\gamma$ as described [above](#gamma). Defaults to <code>1e-3</code>.\n",
    "* <code>epsilon</code>: Sets the starting value of $\\epsilon$ for an $\\epsilon$-greedy policy. Defaults to <code>1</code>.\n",
    "* <code>epsilon_decay</code>: Sets the exponential decay rate for $\\epsilon$. $\\epsilon_{i+1}=\\text{epsilon_decay}*\\epsilon_i$. Defaults to <code>0.99</code>.\n",
    "* <code>epsilon_decay_rate</code>: Sets the rate, in epsidoes, at which to apply the epsilon to decay. When set to 1 this creates a smooth, continuous curve for $\\epsilon$. When set to a larger integer it will create a stair-step function with each stair width being equal to this decay rate. Defaults to <code>1000</code>.\n",
    "* <code>min_epsilon</code>: Sets minimum allowable value of $\\epsilon$ while decaying. Once this minimum is hit it will stop decaying. Defaults to <code>1e-1</code>.\n",
    "* <code>tau</code>: Controls the step size used when updating the target network parameters $\\theta^{-}$ towards the online network $\\theta$. This step is performed at the update rate defined above. Defaults to <code>1e-3</code>.\n",
    "* <code>replay</code>: Determine the type of replay buffer to use. Options: \"uniform\" to sample $U(D)$; \"prioritized\" to sample with priority sampling as described [above](#prioritized_replay). Defaults to <code>uniform</code>.\n",
    "* <code>prioritized_replay_damp</code>: Sets the damping term $\\alpha$ when using prioritized replay as defined by Schaul et al [[1]](#1) where $0 \\leq \\alpha \\leq 1$. Defaults to <code>0.1</code>.\n",
    "* <code>e_constant</code>: Sets constant $e$ when using prioritized replay to ensure samples don't get starved out [[1]](#1). Defaults to <code>0.01</code>.\n",
    "* <code>prioritized_replay_beta_anneal_rate</code>: Sets the $\\beta$ annealing rate, in episodes, when using prioritized replay as defined by Schaul et al [[1]](#1). $\\beta$ starts at 1 and anneals towards zero at the rate defined over training. Defaults to <code>1</code> (i.e. every episode it decays toward zero).\n",
    "* <code>learning_start</code>: The required number of experiences in the buffer before learning will officially start. Defaults to <code>1000</code>.\n",
    "* <code>double_dqn</code>: <code>True</code> to use a [Double DQN](#Double-Q-Networks) approach; <code>False</code> otherwise. Defaults to <code>False</code>.\n",
    "* <code>dueling_dqn</code>: <code>True</code> to use a [Dueling DQN](#Dueling-Q-Networks) approach; <code>False</code> otherwise. Defaults to <code>False</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70533fc-8567-498f-8ac8-81526abf3093",
   "metadata": {},
   "source": [
    "In this project three different experiments are performed:\n",
    "1) Train a network with some good default values (according to open source libraries like [RLLib](https://docs.ray.io/en/master/rllib.html))\n",
    "1) Perform a grid search over the most important set of hyperparameters\n",
    "1) Perform a training run with [RLLib](https://docs.ray.io/en/master/rllib.html)) to act as a reference benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2acbf8-dcfe-42fd-a290-e5a94cdf235f",
   "metadata": {},
   "source": [
    "The results of these three experiments are presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7854d18-29fc-4d47-8a2e-e9c812c1a9f3",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abba867-f8ce-4db6-bb8a-e2a7c527e40a",
   "metadata": {},
   "source": [
    "### Experiment 1 - Good Default Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10fb63-607d-4f17-820c-bf86b780adb5",
   "metadata": {},
   "source": [
    "In this experiment a model is trained with the parameters listed below. Parameters that are not included in the list fallback to the default values listed above. With the default hidden layers set, this is a simple 3-layer feed forward network with the hidden layer having 256 nodes. This architecture is used in all experiments shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66edf2b1-f238-4985-a840-4f1d004bf141",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"episodes\": 5000,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"discount_rate\": 0.99,\n",
    "    \"update_rate\": 4,\n",
    "    \"epsilon_decay\": 0.99,\n",
    "    \"epsilon_decay_rate\": 1,\n",
    "    \"min_epsilon\": 0.01,\n",
    "    \"replay\": \"prioritized\",\n",
    "    \"prioritized_replay_damp\": 0.6,\n",
    "    \"e_constant\": 1e-6,\n",
    "    \"prioritized_replay_beta_anneal_rate\": 100,\n",
    "    \"learning_start\": 64,\n",
    "    \"double_dqn\": True,\n",
    "    \"deuling_dqn\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64994b-f848-4721-a1c4-b04b68921e29",
   "metadata": {},
   "source": [
    "This parameter set is identified as model <i>a</i>. The cumalitive reward of each episode over training and the epsilon values at each episode are shown below:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/exp_1/scores.png\">\n",
    "<img src=\"docs/exp_1/epsilons.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e98c51e-313f-4ffc-aba3-ed3359d2ff29",
   "metadata": {},
   "source": [
    "It is evident looking at this chart that the learning climbs steadily to an average of about 13 points before it plateaus and ceases to make any improvements. It would appear that this stagnation correlates with the epsilon value fully decaying around the same training episode - somewhere between 500 and 1000. Let's explore this correlation further with another training attempt. We'll use all the same parameter values above, but this time we'll set the <code>epsilon_decay</code> to $0.999$. The model trained with this new epsilon decay rate is labeled model <i>b</i>. The cumulative reward and epsilon values at each episode are shown below:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/exp_1/scores_b.png\">\n",
    "<img src=\"docs/exp_1/epsilons_b.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18295c-e8d9-47cd-8443-9564e32b7223",
   "metadata": {},
   "source": [
    "The real test is now using an optimal policy based on the action-value function represented by these models on the banana environment. The evaluation will consist of running 100 concesutive episodes and recording the average score. The goal is to get an average of 13 points or higher. First, the scores of 100 runs with model <i>a</i>:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/exp_1/eval_a.png\">\n",
    "</div>\n",
    "\n",
    "The average score for model <i>a</i> over these 100 test episodes was <b>9.54</b>. The model came close and has clearly learned something, but it has fallen short of the goal of 13. While analyzing the performance of this trained model it became evident that it was confidently able to pick up yellow bananas when they were grouped together (leading to the higher scoring episodes), however, when there were a lot of blue bananas or few yellow bananas it became indecisive and would simply move side to side until the episode ended. It would appear this model was disproportionately afraid of picking up a blue banana even if it meant getting more yellow bananas farther along in the episode.\n",
    "\n",
    "The scores over 100 evaluation episodes with model <i>b</i> are shown below:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/exp_1/eval_b.png\">\n",
    "</div>\n",
    "\n",
    "The average score for model <i>b</i> is <b>15.37</b>. With the slower epsilon decay it has been shown that a model can learn a winning policy to the banana collection game. If readers are interested in trying out this policy for themselves they may check out the repository, follow the setup documented in [README.md](https://github.com/joeworsh/drl_agent_navigation/blob/main/README.md) and run the notebook [DQN Evaluate 2](https://github.com/joeworsh/drl_agent_navigation/blob/main/DQN%20Evaluate%202.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854f49c-ea23-4e93-973b-945cb2301516",
   "metadata": {},
   "source": [
    "### Experiment 2 - Grid Search over Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3b0f8-88a7-4238-a628-0b6e4d2cd66e",
   "metadata": {},
   "source": [
    "Given the vast number of parameter configurations that exist for this problem space it would be impossible to write out every candidate configuration and test it manually. There are, however, many interesting parameter search techniques that can automate this process. For this project a simple [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) was used to try all the different interesting combinations. In particular the grid search was concerned with these parameters and the associated values:\n",
    "* <code>buffer_size</code>: [10000, 50000]\n",
    "* <code>learning_rate</code>: [5e-4, 1e-3, 1e-2]\n",
    "* <code>discount_rate</code>: [0.9, 0.99, 0.999]\n",
    "* <code>update_rate</code>: [4, 10]\n",
    "* <code>epsilon_decay</code>: [0.9, 0.995, 0.999]\n",
    "* <code>min_epsilon</code>: [0.01, 0.1]\n",
    "* <code>replay</code>: [uniform, prioritized]\n",
    "* <code>double_dqn</code>: [False, True]\n",
    "* <code>deuling_dqn</code>: [False, True]\n",
    "\n",
    "Each test in the grid search trained on 1000 episodes. This grid search was a little too ambitious as it in total was requested to evaluate 1,728 configurations with each configuration requiring about an hour to train. Regardless, this grid search was able to explore many of the varied configurations before being terminated. The average scores over 100 evaluation episodes of that grid search are below:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/exp_2/grid_search.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3a64d-e759-497b-87df-e4401a388a97",
   "metadata": {},
   "source": [
    "Readers can see that one experiment in this grid search that was able to come very close to the desired score of 13 in only 1000 trained episodes: <code>exp_7</code>. The hyperparameters for <code>exp_7</code> are shown below:\n",
    "\n",
    "Params:\n",
    "* <code>episodes</code>: 1000\n",
    "* <code>batch_size</code>: 64\n",
    "* <code>buffer_size</code>: 10000\n",
    "* <code>learning_rate</code>: 0.0005\n",
    "* <code>discount_rate</code>: 0.9\n",
    "* <code>update_rate</code>: 4\n",
    "* <code>epsilon_decay</code>: 0.9\n",
    "* <code>epsilon_decay_rate</code>: 1\n",
    "* <code>min_epsilon</code>: 0.01\n",
    "* <code>replay</code>: prioritized\n",
    "* <code>prioritized_replay_damp</code>: 0.6\n",
    "* <code>e_constant</code>: 1e-06\n",
    "* <code>prioritized_replay_beta_anneal_rate</code>: 100\n",
    "* <code>learning_start</code>: 64\n",
    "* <code>double_dqn</code>: True\n",
    "* <code>deuling_dqn</code>: True\n",
    "\n",
    "This configuration achieved an average score of <b>12.7</b> over 100 episodes. It came very close yet still fell slightly short.\n",
    "\n",
    "Other results from the grid search can be seen in the [Analyze Results](https://github.com/joeworsh/drl_agent_navigation/blob/main/Analyze%20Results.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb398435-e61b-44e5-a2d8-1ab78e28ded8",
   "metadata": {},
   "source": [
    "### Experiment 3 - RLLib Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2609f-ec58-42a6-8180-5f825878150a",
   "metadata": {},
   "source": [
    "Finally, as a benchmark test, an experiment is performed using [RLLib](https://docs.ray.io/en/master/rllib.html) which is an industry standard open source library for DRL. All of the default parameters are used with their DQN implementation. The averaged score over training is shown below:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"docs/exp_3/scores.png\">\n",
    "</div>\n",
    "\n",
    "These scores are averaged but appear to follow the same trend as the models in experiment 1. The default parameters for the DQN model can be seen [here](https://docs.ray.io/en/master/rllib-algorithms.html#dqn).\n",
    "\n",
    "This experiment builds confidence in the implementation presented and demonstrates how to use RLLib with a custom environment. The implementation can be seen in the [RLLib Test](https://github.com/joeworsh/drl_agent_navigation/blob/main/RLLib%20Test.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a6d80-aa55-4121-8988-5afa223264e0",
   "metadata": {},
   "source": [
    "## Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc09b9b-9647-4f94-8242-6d88005a556e",
   "metadata": {},
   "source": [
    "A DQN model exists that pulls together all the discovered improvements, including the 3 listed above, called Rainbow DQN [[5]](#5). When using DQN approaches in academia or in industry the standard is to use Rainbow DQN. RLLib also contains a trustworthy implementation that can be used open-source. Another interesting avenue of research would be to compare DQN performance on the Banana Collection game against policy-gradient methods which learn policies via actor-critic paradigms. In practice, policy-gradient methods, such as Soft Actor Critic [[6]](#6) and Proximal Policy Optimization [[7]](#7), out-perform or perform competitively against DQN approaches. Finally, these experiments consumed environment inputs as real-valued state vectors, however, it would be advantageuous to evaluate how well DQN can perform when it takes in image data as input. This would require a different neural network architecture with a CNN input block. Image inputs with CNNs are presented by Schaul et al [[1]](#1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb95b8e-3c10-404e-b320-9e565a555768",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23539719-638e-4836-98f3-63cf61e35cfd",
   "metadata": {},
   "source": [
    "This report presents an overview of the Deep Q-Network and the Reinforcement Learning theory behind it. The repository provides an implementation of the DQN using PyTorch including the concepts of prioritized experience replay, double DQN and dueling DQN. Results are shown training this model on the Unity banana collection environment. With some fairly standard hyperparameter values a model is able to achieve an average score of 15.37 surpassing the goal of 13 in 5000 training episodes. This winning model used both Double DQN and Dueling DQN approaches. Additionally, a bechmark test is included using RLLib to determine the validity of the implementation presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02685269-e7a3-404b-b880-51c18e8577d6",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b7fba-f5bb-46f4-90d1-2fa61f9050dd",
   "metadata": {},
   "source": [
    "<a name=\"1\">[1]</a> Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint arXiv:1511.05952 (2015).\n",
    "\n",
    "<a name=\"2\">[2]</a> Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n",
    "\n",
    "<a name=\"3\">[3]</a> Van Hasselt, Hado, Arthur Guez, and David Silver. \"Deep reinforcement learning with double q-learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30. No. 1. 2016.\n",
    "\n",
    "<a name=\"4\">[4]</a> Wang, Ziyu, et al. \"Dueling network architectures for deep reinforcement learning.\" International conference on machine learning. PMLR, 2016.\n",
    "\n",
    "<a name=\"5\">[5]</a> Hessel, Matteo, et al. \"Rainbow: Combining improvements in deep reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018.\n",
    "\n",
    "<a name=\"6\">[6]</a> Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" International Conference on Machine Learning. PMLR, 2018.\n",
    "\n",
    "<a name=\"7\">[7]</a> Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f2ffe-4347-414b-ae51-f427c509e632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
